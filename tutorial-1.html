<h1>Refinery Tutorial 1</h1>


	<p>Refinery provides a framework for building workers that can process data in a distributed fashion using Amazon&#8217;s web services. It uses the Amazon Simple Queue Service for all messaging requirements and can integrate with S3 easily to access shared data.</p>


	<p>This tutorial provides an introduction to the Refinery framework by walking you through the creation and integration of a distributed worker into a Rails application.</p>


	<p>Before you get started you&#8217;ll need to have some things installed:</p>


	<ol>
	<li>Ruby on Rails (2.2 or higher)</li>
		<li>The refinery gem</li>
		<li>The RightAWS gem</li>
		<li>Image Magick</li>
		<li>The mini_magick gem</li>
		<li>The Moneta gem</li>
		<li>The HTTParty gem</li>
	</ol>


	<p>Additionally you will need to have an Amazon Web Services account with S3 and <span class="caps">SQS</span> activated.</p>


	<h2>Step 1: Create a new Rails application</h2>


	<p>The first thing to do is to create a fresh Rails application. I&#8217;m using Rails 2.3 but you should be able to use any version of Rails 2.2 and higher.</p>


<code>rails refinery_example</code>

	<p>In environment.rb add the following to the <code>Rails::Initializer.run do |config|</code> block:</p>


<pre>
  config.gem 'right_aws'
  config.gem 'right_http_connection'
</pre>

	<h2>Step 2: Install <span class="caps">D2S3</span> Plugin</h2>


	<p>We will use the <a href="http://github.com/mwilliams/d2s3"><span class="caps">D2S3</span> plugin</a> to upload image files directly to Amazon&#8217;s S3, bypassing our web application server. The benefit of this approach is that there is no need for a server process to handle the incoming image data as it will uploaded directly to S3.</p>


<code>script/plugin install git://github.com/mwilliams/d2s3.git</code>

	<h2>Step 3: Create an Asset model</h2>


<code>script/generate model asset filename:string status:string</code>

	<p>This creates an Asset model with a <code>filename</code> attribute and the standard <code>created_at</code> and <code>updated_at</code> attributes that all Rails models get by default. We&#8217;ll come back to the model later to add a little bit of logic, but for now let&#8217;s move on.</p>


	<h2>Step 4: Create an AssetsController</h2>


<code>script generate controller assets index new complete_upload</code>

	<p>Edit the controller and add the following at the top:</p>


<pre>
  skip_before_filter :verify_authenticity_token
</pre>

	<p>This will disable the built in Rails forgery protection thus allowing the <span class="caps">HTTP</span> callback (more later).</p>


	<p>The index action load all of the Asset instances:</p>


<pre>
  def index
    @assets = Asset.find(:all)
  end
</pre>

	<p>The new action will simply display the upload form.</p>


	<p>The complete_upload action:</p>


<pre>
  def complete_upload
    path, filename = File.split(params[:key])
    asset = Asset.create!(:filename =&gt; filename)

    queue = sqs.queue('process_asset_waiting')
    message = {
      "bucket" =&gt; params[:bucket], 
      "s3_key" =&gt; params[:key], 
      "asset_id" =&gt; asset.id
    }.to_json
    queue.send_message(Base64.encode64(message))

    flash[:notice] = 'Your file has been uploaded and is being processed.'
    redirect_to assets_url
  end
</pre>

	<p>The first line splits the key that is passed when the upload completes. This key is the location of the newly uploaded file in Amazon&#8217;s S3. The key is split into its path and filename using <code>File.split</code>. The next line creates a new Asset instance with the filename part.</p>


	<p>The next section of code gets an <span class="caps">SQS</span> Queue instance and constructs a <span class="caps">JSON</span> message and publishes a base-64 encoded version to the queue. The message includes the name of the bucket where the file was uploaded, the key for the file and the asset ID for the asset instance that was just created.</p>


	<p>Finally a flash notice is set and the browser is redirected to the <code>assets_url</code>.</p>


	<p>The complete action will be used as for an <span class="caps">HTTP</span> callback to indicate that the asset has been processed:</p>


<pre>
  def complete
    asset = Asset.find(params[:id])
    asset.complete if asset
    render :nothing =&gt; true
  end
</pre>

	<p>You&#8217;ll also need to add an entry into routes.rb so that the Asset resource and it&#8217;s additional controller actions are available:</p>


<pre>
  map.resources :assets, {
    :collection =&gt; {'complete_upload' =&gt; :get}, 
    :member =&gt; {'complete' =&gt; :post}
  }
</pre>

	<h2>Step 5: Implement the Worker</h2>


	<p>Create a workers directory in the <code>refinery_example</code> directory. This is where you will put the worker for this example. In a larger project you&#8217;ll probably want to separate your Refinery code from your Rails application so that it is easier to distribute across one or more instances. For now though it&#8217;s just as easy to keep it in the same project as your Rails application.</p>


	<p>All workers extend from <code>Refinery::Worker</code> and must implement an <code>execute</code> method that accepts a single argument, <code>message</code>.</p>


<pre>
  class ProcessAsset &lt; Refinery::Worker
    def execute(message)
      # the code to do the work goes here
    end
  end
</pre>

	<p>Inside of your worker you have access to several methods that ease development. You can access a logger at any point by using the <code>logger</code> method. This will return a Ruby <code>Logger</code> instance. You can also access S3 using the <code>data_store</code> method. This method will return an instance of <code>Moneta::S3</code> by default. This is an implementation of the Moneta key/value interface that is backed by S3. More information on Moneta can be found in the <a href="http://github.com/wycats/moneta/">Github project for Moneta</a>.</p>


	<p>Let&#8217;s look at what the body of the execute method is going to look like for this worker:</p>


<pre>
  def execute(message)
    logger.info "Message received: #{message.inspect}" 

    s3_key = message['s3_key']
    bucket = message['bucket']
    asset_id = message['asset_id']

    logger.info "Processing #{s3_key} in bucket #{bucket}" 

    path, file = File.split(s3_key)
    s3_thumb_key = File.join('storage', asset_id.to_s, "thumb-#{file}")
    s3_small_key = File.join('storage', asset_id.to_s, "small-#{file}")
    s3_original_key = File.join('storage', asset_id.to_s, "original-#{file}")

    ds = data_store(:bucket =&gt; bucket)
    image = MiniMagick::Image.from_blob(ds[s3_key], "jpg")

    ds.store(s3_original_key, image.to_blob, :perms =&gt; 'public-read')

    image.resize "100x100" 
    ds.store(s3_small_key, image.to_blob, :perms =&gt; 'public-read')

    image.resize "24x24" 
    ds.store(s3_thumb_key, image.to_blob, :perms =&gt; 'public-read')

    ds.delete(s3_key)

    logger.info "Completed processing #{s3_key}, wrote to #{s3_small_key}" 

    return true
  end
</pre>

	<p>All of the lines with logger.info will print log messages if the logger is set to info level. Let&#8217;s look at the other lines. The following lines extract data from the message:</p>


<pre>
  s3_key = message['s3_key']
  bucket = message['bucket']
  asset_id = message['asset_id']
</pre>

	<p>The next four lines create some key names that will be used for the files after they&#8217;ve been processed:</p>


<pre>
  path, file = File.split(s3_key)
  s3_thumb_key = File.join('storage', asset_id.to_s, "thumb-#{file}")
  s3_small_key = File.join('storage', asset_id.to_s, "small-#{file}")
  s3_original_key = File.join('storage', asset_id.to_s, "original-#{file}")
</pre>

	<p>Next an instance of the data store interface is acquired:</p>


<pre>
  ds = data_store(:bucket =&gt; bucket)
</pre>

	<p>With that data store instance we can get the raw data for the <span class="caps">JPEG</span> image from the S3 bucket and process that data blob with <a href="http://rubyforge.org/projects/mini-magick">MiniMagick</a></p>


<pre>
  image = MiniMagick::Image.from_blob(ds[s3_key], "jpg")
</pre>

	<p>The next lines store the original image, a small version of the image that is 100 pixels by 100 pixels and a thumbnail that is 24 pixels by 24 pixels. Each of the images is stored using the public-read permissions which will allow them to be read directly from S3 later.</p>


<pre>
  ds.store(s3_original_key, image.to_blob, :perms =&gt; 'public-read')

  image.resize "100x100" 
  ds.store(s3_small_key, image.to_blob, :perms =&gt; 'public-read')

  image.resize "24x24" 
  ds.store(s3_thumb_key, image.to_blob, :perms =&gt; 'public-read')
</pre>

	<p>Next, the uploaded raw data is deleted from S3:</p>


<pre>
  ds.delete(s3_key)
</pre>

	<p>Finally an <span class="caps">HTTP</span> post is made to the AssetsController#complete action to indicate that the file has been processed:</p>


<pre>

</pre>

	<p>And the method returns <code>true</code>. Returning <code>true</code> is important as it causes the Refinery engine to delete the message from <span class="caps">SQS</span>.</p>


	<h2>Step 6: Uploading the Raw Image</h2>


	<p>To provide the interface to upload the image, we add the following code to <code>views/assets/new.html.erb</code>:</p>


<pre>
  &lt;p&gt;Upload JPEG image:&lt;/p&gt;

  &lt;%= s3_http_upload_tag 
        :key =&gt; 'uploads', 
        :content_type =&gt; 'image/jpeg', 
        :redirect =&gt; complete_upload_assets_url,
        :acl =&gt; 'private',
        :max_filesize =&gt; 5.megabytes %&gt;
</pre>

	<p>This helper comes from the <span class="caps">D2S3</span> project. The options used here include:</p>


	<ul>
	<li><tt>:key</tt>: The prefix to the S3 key. The filename will be appended to this prefix.</li>
		<li><tt>:content_type</tt>: The content type for the uploaded file. In this case we&#8217;ll only accept <span class="caps">JPEG</span> images.</li>
		<li><tt>:redirect</tt>: The destination that will be redirected to following the upload.</li>
		<li><tt>:acl</tt>: The access control on the uploaded file.</li>
		<li><tt>:max_filesize</tt>: The maximum file size that is allowed.</li>
	</ul>


	<p>There are other options as well. Take a look at <code>d2s3/view_helpers.rb</code> in the d2s3 project&#8217;s lib directory for more options.</p>


	<h2>Step 7: Displaying the Processed Image</h2>


	<p>Let&#8217;s go back over to the Rails app and look at the views. First let&#8217;s look at <code>views/assets/index.html.erb</code>:</p>


<pre>
  &lt;%  if flash[:notice] -%&gt;
  &lt;div class="notice"&gt;&lt;%= flash[:notice] %&gt;&lt;/div&gt;
  &lt;%  end -%&gt;

  &lt;%= link_to 'Add an Asset', new_asset_url %&gt;

  &lt;ul&gt;
  &lt;%  @assets.each do |asset| -%&gt;
    &lt;li&gt;&lt;%= asset.done? ? image_tag(asset.small_url) : 'Processing' %&gt;&lt;/li&gt;
  &lt;%  end -%&gt;
  &lt;/ul&gt;
</pre>

	<p>There&#8217;s not much going on here. The first 3 lines display the flash notice if necessary, the next line links to the new assets action and the remaining lines loop through the assets and render the small version of the image if the asset is done processing.</p>


	<p>The Asset model provides the last bit of logic to help construct the small image url and to update a status value upon processing completion:</p>


<pre>
  class Asset &lt; ActiveRecord::Base
    def small_url
      "https://#{bucket}.s3.amazonaws.com/storage/#{id}/small-#{filename}" 
    end

    def complete
      update_attribute(:status, 'done')
    end

    def done?
      status == 'done'
    end

    private
    def bucket
      D2S3::S3Config.bucket
    end
  end
</pre>

	<p>To keep things simple we just use the <code>D2S3::S3Config</code> class to get the bucket instance. We then construct a <span class="caps">URL</span> directly to the resource on S3. Once the image has been processed it will appear in the list.</p>


	<h2>Step 8: Make it Run!</h2>


	<p>The final step is to create a refinery configuration file, start the Refinery and start up the a server for Rails.</p>


	<p>Create a file called refinery.yml in the config directory. Include the following text:</p>


<pre>
aws:
  credentials:
    access_key_id: "ACCESS_KEY_ID" 
    secret_access_key: "SECRET_ACCESS_KEY" 
processors:
  process_asset:
    publishers:
    workers:
      initial: 1
</pre>

	<p>Replace <span class="caps">ACCESS</span>_KEY_ID with your Amazon Web Services access key ID and <span class="caps">SECRET</span>_ACCESS_KEY with your Amazon Web Services secret access key. The processors section defines each of the processors that are deployed in your Refinery instance.</p>


	<p>From within the refinery_example directory execute the following command: <code>refinery -c config/refinery.yml -w workers -d</code></p>


	<p>The command options provide the path to the config/refinery.yml file (using -c or&#8212;config) the path to the workers directory (using -w or&#8212;workers) and indicate that the logging level is debug (using -d or&#8212;debug). Once you&#8217;ve run the refinery command you should have a running refinery instance.</p>


	<p>Next start up your Rails instance in another terminal using the command <code>script/server</code>.</p>


	<p>Finally, open a web browser and navigate to <a href="http://localhost:3000/assets">http://localhost:3000/assets</a>. Click on &#8220;Add an Asset&#8221; to go to the new asset screen. Upload a <span class="caps">JPEG</span> file. Upon completion of the upload you&#8217;ll be returned to the assets index. You will most likely see the word &#8220;Processing&#8221; next to the first bullet item. Take a look at the terminal where refinery is running and you should see log messages telling you about refinery&#8217;s activity. Once refinery receives the message posted by your Rails app it will process the data, which you&#8217;ll see in the logs. Once it has processed the image, go back to your Rails app and reload the assets index and you will see the small version of the image you uploaded.</p>


	<p>Keep in mind that at no time did the binary data need to go through your application server: it was uploaded directly to S3 and processed without requiring any sort of blocking on your server.</p>


	<h2>Conclusion</h2>


	<p>I hope this has given you a taste of what you can do with Refinery. In the next tutorial I will demonstrate how you can use Refinery publishers to easily publish messages using a scheduled cron job. Thanks for using Refinery!</p>


	<p>For additional information on Refinery please visit <a href="http://github.com/aeden/refinery">http://github.com/aeden/refinery</a></p>
